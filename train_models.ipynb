{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RForestRegress\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import argv, path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "default_input_dir = \"/home/alex/Documents/Development/MLSchool2015/data/hackathon/\" #\"/home/ubuntu/Data\"\n",
    "default_output_dir = \"res\"\n",
    "\n",
    "import os\n",
    "running_on_codalab = False\n",
    "run_dir = os.path.abspath(\".\")\n",
    "codalab_run_dir = os.path.join(run_dir, \"program\")\n",
    "if os.path.isdir(codalab_run_dir): \n",
    "    run_dir=codalab_run_dir\n",
    "    running_on_codalab = True\n",
    "    print \"Running on Codalab!\"\n",
    "lib_dir = os.path.join(run_dir, \"lib\")\n",
    "res_dir = os.path.join(run_dir, \"res\")\n",
    "\n",
    "# Our libraries  \n",
    "path.append (run_dir)\n",
    "path.append (lib_dir)\n",
    "import data_io                       # general purpose input/output functions\n",
    "from data_io import vprint           # print only in verbose mode\n",
    "from data_manager import DataManager # load/save data and get info about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datanames = data_io.inventory_data(default_input_dir)\n",
    "verbose = True\n",
    "debug_mode = 0\n",
    "zipme = True\n",
    "max_time = 90\n",
    "max_cycle = 1\n",
    "execution_success = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "****** Attempting to copy files (from res/) for RESULT submission ******\n",
      "************************************************************************\n",
      "[-] Missing 'test' result files for christine\n",
      "======== Some missing results on current datasets!\n",
      "======== Proceeding to train/test:\n",
      "\n",
      "************************************************\n",
      "******** Processing dataset Christine ********\n",
      "************************************************\n",
      "======== Reading and converting data ==========\n",
      "Info file found : /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_public.info\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_feat.type\n",
      "[+] Success in  0.00 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_train.data\n",
      "[+] Success in  1.68 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_train.solution\n",
      "[+] Success in  0.01 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_valid.data\n",
      "[+] Success in  0.24 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/christine/christine_test.data\n",
      "[+] Success in  0.61 sec\n",
      "DataManager : christine\n",
      "info:\n",
      "\ttask = binary.classification\n",
      "\tname = christine\n",
      "\tfeat_type = Numerical\n",
      "\tformat = dense\n",
      "\tis_sparse = 0\n",
      "\tmetric = bac_metric\n",
      "\ttarget_type = Binary\n",
      "\ttest_num = 2084\n",
      "\tlabel_num = 2\n",
      "\ttarget_num = 1\n",
      "\tvalid_num = 834\n",
      "\thas_categorical = 0\n",
      "\tusage = AutoML challenge 2015\n",
      "\tfeat_num = 1636\n",
      "\ttime_budget = 1200\n",
      "\ttrain_num = 5418\n",
      "\thas_missing = 0\n",
      "data:\n",
      "\tX_train = array(5418, 1636)\n",
      "\tY_train = array(5418,)\n",
      "\tX_valid = array(834, 1636)\n",
      "\tX_test = array(2084, 1636)\n",
      "feat_type:\tarray(1636,)\n",
      "feat_idx:\tarray(1636,)\n",
      "\n",
      "[+] Remaining time after reading data 1197.36 sec\n",
      "[+] Remaining time after building model 1197.36 sec\n",
      "=========== Christine Training cycle 0 ================\n",
      "[+] Number of estimators: 10\n",
      "[+] Fitting success, time spent so far  1.74 sec\n",
      "[ 0.8  0.4  0.5  0.8  0.   0.5  0.1  0.6  0.3  0.6  0.5  0.7  0.2  0.6  0.4\n",
      "  0.9  0.8  0.8  0.1  0.6  0.8  0.4  0.7  0.7  0.4  0.7  0.1  0.3  0.4  0.6\n",
      "  0.6  0.   0.5  0.6  0.3  0.4  0.5  0.4  0.3  0.6  0.2  0.7  0.7  0.3  0.7\n",
      "  0.9  0.2  0.8  0.6  0.7  0.5  0.8  0.5  0.8  0.3  0.3  0.9  0.9  0.8  0.5\n",
      "  0.5  0.3  0.5  0.6  0.8  0.5  0.6  0.2  1.   0.6  0.3  0.4  0.1  0.7  0.9\n",
      "  0.8  0.4  0.4  0.6  0.   0.3  0.2  0.2  0.2  0.1  0.3  0.3  0.4  0.8  0.8\n",
      "  0.5  0.3  0.5  0.7  0.5  0.1  0.2  0.8  0.6  0.7  0.6  0.4  0.5  0.4  0.8\n",
      "  0.6  0.2  0.5  0.3  0.1  0.1  0.4  0.9  0.1  0.5  0.1  0.8  0.4  0.6  0.6\n",
      "  1.   0.2  0.4  0.7  0.6  0.7  0.7  0.5  0.8  0.7  0.4  0.4  0.3  0.4  1.\n",
      "  0.7  0.8  0.6  0.6  0.4  0.8  0.9  0.6  0.6  0.1  0.3  0.3  0.9  0.8  0.7\n",
      "  0.7  0.7  0.4  0.7  0.3  0.4  0.9  0.8  0.5  0.6  0.9  0.5  0.2  0.2  0.6\n",
      "  0.4  0.8  0.   0.3  0.3  0.6  0.6  0.7  0.7  0.5  0.7  0.3  0.8  0.2  0.6\n",
      "  0.8  0.7  0.1  0.4  0.4  0.6  0.4  0.1  0.5  0.4  0.9  0.6  0.6  0.4  0.5\n",
      "  0.9  0.3  0.8  0.7  0.7  0.6  0.3  0.4  0.7  0.1  0.8  0.3  0.4  0.8  0.1\n",
      "  0.3  0.7  0.5  0.2  0.1  0.6  0.3  0.5  0.5  0.4  0.4  0.5  0.6  0.2  0.7\n",
      "  0.7  0.6  0.2  0.1  0.7  0.3  0.5  0.4  0.8  0.4  0.8  0.6  0.3  0.7  0.8\n",
      "  0.6  0.2  0.6  0.7  0.3  0.1  0.7  0.9  0.1  0.8  0.4  0.2  0.5  0.9  0.4\n",
      "  0.4  0.3  0.4  0.9  0.3  0.1  0.3  0.4  0.1  0.6  0.6  0.9  0.2  0.1  0.5\n",
      "  0.3  0.7  1.   0.7  0.8  0.4  0.4  0.7  0.2  0.3  0.6  0.2  0.3  1.   0.5\n",
      "  0.1  0.7  0.5  0.7  0.9  0.7  0.4  0.2  0.7  0.5  0.5  0.7  0.6  1.   1.\n",
      "  0.1  0.3  0.7  0.4  1.   0.3  0.5  0.2  0.5  1.   0.5  0.8  0.7  0.8  0.6\n",
      "  0.3  1.   0.3  0.4  0.7  0.6  0.5  0.2  0.5  0.8  0.5  0.4  0.6  0.6  0.5\n",
      "  0.9  0.3  0.3  0.3  0.6  0.5  0.7  0.7  0.5  0.4  0.5  0.6  0.7  0.3  0.1\n",
      "  0.4  0.2  0.5  0.9  0.8  0.4  0.5  0.3  0.5  0.8  0.6  0.1  0.2  0.5  0.4\n",
      "  0.8  0.2  0.6  0.3  0.6  0.2  0.4  0.5  0.5  0.7  0.6  0.3  0.6  0.5  0.8\n",
      "  0.3  0.7  0.6  0.1  0.   0.7  0.6  0.6  0.4  0.7  0.6  0.7  0.7  0.4  1.\n",
      "  0.7  0.3  0.3  0.5  0.5  0.4  0.1  0.6  0.6  0.6  0.5  0.9  0.7  0.9  0.6\n",
      "  0.2  0.2  0.7  0.7  1.   0.4  0.2  0.9  0.2  0.7  0.5  0.7  0.5  0.1  0.3\n",
      "  0.7  0.9  0.2  0.4  0.4  0.6  0.2  0.5  0.1  0.7  0.6  0.1  0.9  0.6  0.2\n",
      "  0.6  0.5  0.4  0.3  0.8  0.3  0.2  0.9  0.7  0.3  0.2  0.5  0.6  0.1  0.3\n",
      "  0.2  0.6  0.4  0.2  0.1  0.4  0.2  0.7  0.4  0.4  0.5  0.5  0.5  0.9  0.9\n",
      "  0.2  0.7  0.4  0.7  0.5  0.8  0.6  0.4  0.5  0.6  0.5  0.5  0.7  0.5  0.5\n",
      "  1.   0.1  1.   0.5  0.6  0.8  0.7  0.5  0.3  0.7  0.5  0.5  0.   0.8  0.5\n",
      "  0.3  0.3  0.3  0.7  0.4  0.5  0.2  0.8  0.2  0.4  0.7  0.3  0.   0.7  0.9\n",
      "  0.3  0.7  0.3  0.1  1.   0.6  0.3  0.3  0.4  0.4  0.6  0.7  0.9  0.4  0.3\n",
      "  0.3  0.1  0.8  0.5  0.5  0.4  0.1  0.2  0.4  0.6  0.4  0.6  0.7  0.5  0.7\n",
      "  0.3  0.2  0.4  0.6  0.5  0.5  0.8  0.4  0.2  0.4  0.8  0.7  0.6  0.7  0.\n",
      "  0.6  0.3  0.2  0.8  0.2  0.7  0.9  0.4  0.1  0.8  0.5  0.3  0.3  0.5  0.1\n",
      "  0.8  0.8  0.   0.3  0.6  0.2  0.3  0.4  0.7  0.6  0.2  0.3  0.6  0.   0.9\n",
      "  0.4  0.9  0.5  0.5  0.1  0.5  0.6  0.6  0.3  0.5  0.7  0.4  0.5  0.   0.5\n",
      "  0.2  0.3  0.6  0.7  0.4  0.6  0.6  0.2  0.6  0.2  0.1  0.2  1.   0.3  0.9\n",
      "  0.6  0.5  0.4  0.2  0.4  0.1  0.9  0.4  0.5  0.9  0.8  0.1  0.8  0.5  0.4\n",
      "  0.7  0.2  0.   0.7  0.7  0.8  0.5  0.3  0.6  1.   0.6  0.7  0.8  0.7  0.3\n",
      "  0.3  0.8  0.2  0.7  0.9  0.6  0.1  0.7  0.4  0.8  0.2  0.2  0.5  0.5  0.3\n",
      "  0.5  0.6  0.1  0.6  1.   0.5  0.5  0.7  0.6  0.3  0.1  0.5  0.5  0.9  0.5\n",
      "  0.3  0.3  0.6  0.6  0.9  1.   0.2  0.3  0.8  0.5  0.7  0.2  0.6  1.   0.5\n",
      "  0.5  0.9  0.3  0.2  0.7  0.8  0.8  0.7  0.6  0.9  0.9  0.4  0.9  0.3  0.6\n",
      "  0.8  0.   0.4  0.3  0.3  0.8  0.7  0.6  0.1  0.9  0.3  0.4  0.7  0.6  0.1\n",
      "  0.3  0.8  0.5  0.7  0.1  0.9  0.5  0.4  0.7  0.8  0.3  0.4  0.3  0.7  0.2\n",
      "  0.8  0.4  0.3  0.7  0.4  0.6  0.7  0.6  0.3  0.6  0.6  0.2  0.5  0.8  0.1\n",
      "  0.4  0.5  0.6  0.3  0.1  0.8  0.3  0.7  0.2  0.9  0.7  0.3  0.5  0.6  0.3\n",
      "  0.2  0.4  1.   0.6  0.3  0.7  0.1  0.7  0.8  0.3  0.3  0.2  0.7  0.7  0.1\n",
      "  0.5  0.9  0.5  0.3  0.4  0.6  0.6  0.4  0.9  0.2  0.8  0.5  0.4  0.3  0.2\n",
      "  0.3  0.6  0.1  0.5  0.7  0.5  0.5  0.2  0.8  0.9  0.8  0.6  0.5  0.3  0.6\n",
      "  0.7  0.6  0.4  0.2  0.3  0.3  0.8  0.8  0.9  0.3  0.4  0.4  0.3  0.5  0.9\n",
      "  0.5  0.8  0.6  0.2  0.3  0.5  0.7  0.5  0.6] [ 0.5  0.4  0.4 ...,  0.3  0.8  0.8]\n",
      "[+] Prediction success, time spent so far  1.78 sec\n",
      "[+] Results saved, time spent so far  1.79 sec\n",
      "[+] End cycle, remaining time 1195.58 sec\n",
      "=========== Christine Training cycle 1 ================\n",
      "[+] Number of estimators: 1336\n",
      "[+] Fitting success, time spent so far 219.35 sec\n",
      "[ 0.74176647  0.45134731  0.34505988  0.61601796  0.24326347  0.57260479\n",
      "  0.18562874  0.49176647  0.40568862  0.41691617  0.65568862  0.6002994\n",
      "  0.23652695  0.4004491   0.31586826  0.83458084  0.7994012   0.69685629\n",
      "  0.17065868  0.66467066  0.56661677  0.3495509   0.78443114  0.75224551\n",
      "  0.60628743  0.72979042  0.36377246  0.39595808  0.34580838  0.42889222\n",
      "  0.52694611  0.30164671  0.42065868  0.64446108  0.37724551  0.36302395\n",
      "  0.38997006  0.31661677  0.32784431  0.4752994   0.23203593  0.68562874\n",
      "  0.69311377  0.31811377  0.5501497   0.67814371  0.17065868  0.80988024\n",
      "  0.7252994   0.6504491   0.50748503  0.875       0.4992515   0.91017964\n",
      "  0.38547904  0.25449102  0.56961078  0.90943114  0.81362275  0.47230539\n",
      "  0.37275449  0.22754491  0.38323353  0.53293413  0.6257485   0.28517964\n",
      "  0.73203593  0.46332335  0.87874251  0.40643713  0.25523952  0.32784431\n",
      "  0.24176647  0.73428144  0.84505988  0.77170659  0.21257485  0.2507485\n",
      "  0.6991018   0.30613772  0.21706587  0.36826347  0.24625749  0.27095808\n",
      "  0.22754491  0.51047904  0.56362275  0.5508982   0.78517964  0.53068862\n",
      "  0.27245509  0.33907186  0.63098802  0.84356287  0.59131737  0.30538922\n",
      "  0.34281437  0.72230539  0.47679641  0.73428144  0.47305389  0.34805389\n",
      "  0.48802395  0.40643713  0.81736527  0.68263473  0.35928144  0.35254491\n",
      "  0.32335329  0.30538922  0.23577844  0.40943114  0.7238024   0.38772455\n",
      "  0.60179641  0.18862275  0.7252994   0.5501497   0.48802395  0.68862275\n",
      "  0.93488024  0.19461078  0.71257485  0.74101796  0.50149701  0.64371257\n",
      "  0.42664671  0.47679641  0.41991018  0.8001497   0.29790419  0.3502994\n",
      "  0.16541916  0.31961078  0.77994012  0.4505988   0.29865269  0.35254491\n",
      "  0.46706587  0.44311377  0.74026946  0.66541916  0.66167665  0.42215569\n",
      "  0.17140719  0.28817365  0.3989521   0.89520958  0.875       0.44311377\n",
      "  0.64520958  0.5494012   0.57709581  0.5988024   0.25224551  0.54491018\n",
      "  0.87649701  0.60329341  0.59655689  0.7507485   0.82185629  0.55464072\n",
      "  0.34131737  0.23428144  0.48502994  0.39595808  0.86751497  0.39520958\n",
      "  0.34281437  0.30239521  0.75973054  0.87200599  0.78592814  0.58458084\n",
      "  0.33308383  0.6991018   0.37125749  0.67889222  0.4505988   0.1998503\n",
      "  0.69236527  0.43413174  0.33607784  0.26946108  0.26272455  0.44760479\n",
      "  0.375       0.23727545  0.53443114  0.50449102  0.81586826  0.41841317\n",
      "  0.55763473  0.55763473  0.51796407  0.8255988   0.23128743  0.63772455\n",
      "  0.59505988  0.56661677  0.38023952  0.16916168  0.52844311  0.77095808\n",
      "  0.25898204  0.6744012   0.34281437  0.40943114  0.65568862  0.23203593\n",
      "  0.21107784  0.65194611  0.53892216  0.39221557  0.21107784  0.81137725\n",
      "  0.3495509   0.31586826  0.39520958  0.45733533  0.20134731  0.4251497\n",
      "  0.69011976  0.39071856  0.80688623  0.8241018   0.49401198  0.28817365\n",
      "  0.41991018  0.59431138  0.28892216  0.70508982  0.30988024  0.92215569\n",
      "  0.58458084  0.33607784  0.73203593  0.37724551  0.31287425  0.89071856\n",
      "  0.61826347  0.3742515   0.44835329  0.63622754  0.15494012  0.53293413\n",
      "  0.71706587  0.75        0.13398204  0.6758982   0.33158683  0.42664671\n",
      "  0.2754491   0.75973054  0.4760479   0.36901198  0.30164671  0.25149701\n",
      "  0.90793413  0.33982036  0.33083832  0.17664671  0.45284431  0.17065868\n",
      "  0.6242515   0.63997006  0.87799401  0.15494012  0.30164671  0.45883234\n",
      "  0.58233533  0.5494012   0.84131737  0.65568862  0.75374251  0.35553892\n",
      "  0.45284431  0.53592814  0.29790419  0.32784431  0.64820359  0.34356287\n",
      "  0.26571856  0.89520958  0.68712575  0.17889222  0.58008982  0.75449102\n",
      "  0.64071856  0.69535928  0.59056886  0.38173653  0.28817365  0.69086826\n",
      "  0.36601796  0.47230539  0.80913174  0.3757485   0.86077844  0.88173653\n",
      "  0.20808383  0.47230539  0.58757485  0.25299401  0.6758982   0.46482036\n",
      "  0.52994012  0.28143713  0.36601796  0.86676647  0.4258982   0.85479042\n",
      "  0.9004491   0.86227545  0.68413174  0.20434132  0.7994012   0.30763473\n",
      "  0.45583832  0.50449102  0.78592814  0.50224551  0.41916168  0.80913174\n",
      "  0.89371257  0.54790419  0.25149701  0.59431138  0.29640719  0.83682635\n",
      "  0.70658683  0.55763473  0.26946108  0.24625749  0.69535928  0.33682635\n",
      "  0.82934132  0.69535928  0.63772455  0.34056886  0.67215569  0.70284431\n",
      "  0.68413174  0.20583832  0.23877246  0.44685629  0.31287425  0.40718563\n",
      "  0.8255988   0.76497006  0.33607784  0.40793413  0.24176647  0.58383234\n",
      "  0.85553892  0.71556886  0.28218563  0.22305389  0.32934132  0.65194611\n",
      "  0.53218563  0.23652695  0.49326347  0.37874251  0.42290419  0.24251497\n",
      "  0.38473054  0.45359281  0.50823353  0.66766467  0.57709581  0.35628743\n",
      "  0.47979042  0.52320359  0.86377246  0.26272455  0.40194611  0.75748503\n",
      "  0.15344311  0.33308383  0.73877246  0.43113772  0.42964072  0.47829341\n",
      "  0.65868263  0.70958084  0.7005988   0.56437126  0.44086826  0.87649701\n",
      "  0.63398204  0.30239521  0.25374251  0.43712575  0.42739521  0.39670659\n",
      "  0.2747006   0.62275449  0.36676647  0.50149701  0.43413174  0.8502994\n",
      "  0.55164671  0.73877246  0.48577844  0.20883234  0.15419162  0.79491018\n",
      "  0.43263473  0.88173653  0.61077844  0.26571856  0.77919162  0.14595808\n",
      "  0.5755988   0.41167665  0.44386228  0.34580838  0.20583832  0.27320359\n",
      "  0.65718563  0.83008982  0.25748503  0.45284431  0.23353293  0.53368263\n",
      "  0.27020958  0.34356287  0.21107784  0.58682635  0.47904192  0.21107784\n",
      "  0.8255988   0.49550898  0.2245509   0.45434132  0.35778443  0.43787425\n",
      "  0.35179641  0.63023952  0.4505988   0.30389222  0.67739521  0.79116766\n",
      "  0.49401198  0.36227545  0.47679641  0.36302395  0.2245509   0.52994012\n",
      "  0.37799401  0.73278443  0.38473054  0.29640719  0.37050898  0.52769461\n",
      "  0.33308383  0.84431138  0.62275449  0.43338323  0.49401198  0.30763473\n",
      "  0.44236527  0.84730539  0.80838323  0.31437126  0.57709581  0.21931138\n",
      "  0.65344311  0.28667665  0.64071856  0.76422156  0.45808383  0.8008982\n",
      "  0.46182635  0.4004491   0.37949102  0.52095808  0.40718563  0.68413174\n",
      "  0.8001497   0.25523952  0.7754491   0.31811377  0.68413174  0.53817365\n",
      "  0.58158683  0.3255988   0.41766467  0.50374251  0.77320359  0.41991018\n",
      "  0.17814371  0.70958084  0.59056886  0.21407186  0.3488024   0.19835329\n",
      "  0.58308383  0.27320359  0.27994012  0.41541916  0.51347305  0.50748503\n",
      "  0.3001497   0.81362275  0.37799401  0.33083832  0.75149701  0.68712575\n",
      "  0.18263473  0.80239521  0.22679641  0.23203593  0.89296407  0.71781437\n",
      "  0.20209581  0.46257485  0.43338323  0.42964072  0.59655689  0.55464072\n",
      "  0.82185629  0.3241018   0.40494012  0.3248503   0.28967066  0.80538922\n",
      "  0.54341317  0.53517964  0.18787425  0.2747006   0.48802395  0.61077844\n",
      "  0.42215569  0.30314371  0.48577844  0.50149701  0.31886228  0.45209581\n",
      "  0.36676647  0.28892216  0.42889222  0.73128743  0.52844311  0.45808383\n",
      "  0.81437126  0.66691617  0.17889222  0.33233533  0.73952096  0.71556886\n",
      "  0.44086826  0.59655689  0.28293413  0.55464072  0.28667665  0.23952096\n",
      "  0.76721557  0.24700599  0.54191617  0.81437126  0.50748503  0.16392216\n",
      "  0.9258982   0.53742515  0.40943114  0.27245509  0.65943114  0.24550898\n",
      "  0.91167665  0.79565868  0.22829341  0.17365269  0.6758982   0.47305389\n",
      "  0.46556886  0.4011976   0.59356287  0.71556886  0.19086826  0.39595808\n",
      "  0.46257485  0.23577844  0.61227545  0.39820359  0.63098802  0.41541916\n",
      "  0.75224551  0.2739521   0.56586826  0.58682635  0.31661677  0.3008982\n",
      "  0.57784431  0.43488024  0.51197605  0.45808383  0.17664671  0.39745509\n",
      "  0.38922156  0.53667665  0.54865269  0.53592814  0.26796407  0.60329341\n",
      "  0.62125749  0.3502994   0.45808383  0.19011976  0.25598802  0.25598802\n",
      "  0.92140719  0.15419162  0.81586826  0.49401198  0.43188623  0.60179641\n",
      "  0.25224551  0.37050898  0.28443114  0.71332335  0.33682635  0.4745509\n",
      "  0.88473054  0.84056886  0.20284431  0.66242515  0.53293413  0.65868263\n",
      "  0.62200599  0.19835329  0.23877246  0.60703593  0.68413174  0.60479042\n",
      "  0.32634731  0.29116766  0.41167665  0.60404192  0.6998503   0.71107784\n",
      "  0.87949102  0.68038922  0.3502994   0.36227545  0.5988024   0.20658683\n",
      "  0.52694611  0.80164671  0.55164671  0.27844311  0.43862275  0.2252994\n",
      "  0.59356287  0.35778443  0.28517964  0.5239521   0.31362275  0.33832335\n",
      "  0.38323353  0.54715569  0.20434132  0.66916168  0.88697605  0.49326347\n",
      "  0.5254491   0.61377246  0.61976048  0.32035928  0.16616766  0.53068862\n",
      "  0.57709581  0.73577844  0.34580838  0.25748503  0.29416168  0.63922156\n",
      "  0.69685629  0.80538922  0.87200599  0.23053892  0.50299401  0.62125749\n",
      "  0.44835329  0.66167665  0.37649701  0.64670659  0.84131737  0.61227545\n",
      "  0.58832335  0.58832335  0.48428144  0.36901198  0.74850299  0.63098802\n",
      "  0.86077844  0.87050898  0.76122754  0.75973054  0.79416168  0.45583832\n",
      "  0.89820359  0.2507485   0.47979042  0.81811377  0.18637725  0.21856287\n",
      "  0.29790419  0.4505988   0.76422156  0.54191617  0.50673653  0.38622754\n",
      "  0.33982036  0.76422156  0.31062874  0.7252994   0.80688623  0.34056886\n",
      "  0.38098802  0.68263473  0.56062874  0.62275449  0.32634731  0.73428144\n",
      "  0.25374251  0.46856287  0.41916168  0.79565868  0.36676647  0.28892216\n",
      "  0.38847305  0.47080838  0.23502994  0.83458084  0.26946108  0.33083832\n",
      "  0.50898204  0.63173653  0.60553892  0.61452096  0.47230539  0.43488024\n",
      "  0.34580838  0.65344311  0.17889222  0.57110778  0.90568862  0.21556886\n",
      "  0.48727545  0.26946108  0.70583832  0.46182635  0.59281437  0.86751497\n",
      "  0.48428144  0.75898204  0.21407186  0.74850299  0.60553892  0.20134731\n",
      "  0.52694611  0.66317365  0.36002994  0.4498503   0.51122754  0.8510479\n",
      "  0.36227545  0.25449102  0.69835329  0.32859281  0.65643713  0.75374251\n",
      "  0.45508982  0.51721557  0.32784431  0.59730539  0.59281437  0.27095808\n",
      "  0.64371257  0.81886228  0.21107784  0.6504491   0.46182635  0.5741018\n",
      "  0.53742515  0.32634731  0.89146707  0.27170659  0.81437126  0.4505988\n",
      "  0.55988024  0.25973054  0.38173653  0.40269461  0.57260479  0.18113772\n",
      "  0.43488024  0.83008982  0.42140719  0.6489521   0.28143713  0.75449102\n",
      "  0.79266467  0.875       0.30763473  0.48203593  0.29565868  0.53293413\n",
      "  0.58907186  0.43263473  0.66916168  0.34056886  0.28592814  0.3989521\n",
      "  0.6751497   0.78967066  0.85928144  0.1758982   0.44311377  0.48577844\n",
      "  0.34056886  0.50598802  0.875       0.41242515  0.83982036  0.66841317\n",
      "  0.25598802  0.36901198  0.47080838  0.69236527  0.50598802  0.31287425] [ 0.2739521   0.4505988   0.3255988  ...,  0.26646707  0.81437126\n",
      "  0.88997006]\n",
      "[+] Prediction success, time spent so far 220.82 sec\n",
      "[+] Results saved, time spent so far 220.82 sec\n",
      "[+] End cycle, remaining time 974.75 sec\n",
      "************************************************\n",
      "******** Processing dataset Jasmine ********\n",
      "************************************************\n",
      "======== Reading and converting data ==========\n",
      "Info file found : /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_public.info\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_feat.type\n",
      "[+] Success in  0.00 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_train.data\n",
      "[+] Success in  0.08 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_train.solution\n",
      "[+] Success in  0.01 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_valid.data\n",
      "[+] Success in  0.03 sec\n",
      "========= Reading /home/alex/Documents/Development/MLSchool2015/data/hackathon/jasmine/jasmine_test.data\n",
      "[+] Success in  0.06 sec\n",
      "DataManager : jasmine\n",
      "info:\n",
      "\ttask = binary.classification\n",
      "\tname = jasmine\n",
      "\tfeat_type = Numerical\n",
      "\tformat = dense\n",
      "\tis_sparse = 0\n",
      "\tmetric = bac_metric\n",
      "\ttarget_type = Binary\n",
      "\ttest_num = 1756\n",
      "\tlabel_num = 2\n",
      "\ttarget_num = 1\n",
      "\tvalid_num = 526\n",
      "\thas_categorical = 0\n",
      "\tusage = AutoML challenge 2015\n",
      "\tfeat_num = 144\n",
      "\ttime_budget = 1200\n",
      "\ttrain_num = 2984\n",
      "\thas_missing = 0\n",
      "data:\n",
      "\tX_train = array(2984, 144)\n",
      "\tY_train = array(2984,)\n",
      "\tX_valid = array(526, 144)\n",
      "\tX_test = array(1756, 144)\n",
      "feat_type:\tarray(144,)\n",
      "feat_idx:\tarray(144,)\n",
      "\n",
      "[+] Remaining time after reading data 1199.80 sec\n",
      "[+] Remaining time after building model 1199.80 sec\n",
      "=========== Jasmine Training cycle 0 ================\n",
      "[+] Number of estimators: 10\n",
      "[+] Fitting success, time spent so far  0.05 sec\n",
      "[ 0.9  0.8  0.   0.8  0.   0.7  0.5  0.9  0.6  0.4  0.8  0.   0.9  0.8  0.8\n",
      "  0.1  0.   0.8  0.   0.9  0.   0.   0.8  0.8  0.   1.   0.   0.2  0.9  0.7\n",
      "  0.   0.6  0.   0.8  0.8  0.8  0.   0.8  0.7  0.6  0.7  0.7  0.6  0.3  0.6\n",
      "  0.6  0.6  0.   0.9  0.2  0.6  0.5  0.7  0.9  0.7  0.4  0.5  0.   1.   0.5\n",
      "  0.8  0.8  0.5  0.   0.2  0.4  0.7  0.   0.6  0.2  0.9  0.1  0.3  1.   0.4\n",
      "  0.6  0.   0.7  0.6  0.   0.9  0.3  0.8  0.7  0.2  1.   0.3  0.   0.   0.\n",
      "  0.   0.9  0.8  0.7  0.1  0.7  0.2  0.   0.7  0.5  1.   0.8  0.8  0.   0.8\n",
      "  0.   1.   0.   0.6  0.5  0.8  0.   0.9  0.   0.7  0.   0.4  0.1  0.9  0.4\n",
      "  0.   0.5  0.   0.   0.   0.   0.   0.8  0.9  0.   0.9  0.6  0.8  1.   1.\n",
      "  0.7  0.7  0.   0.   0.   0.   0.7  0.   0.8  0.5  0.1  1.   0.1  0.8  0.8\n",
      "  0.5  0.5  1.   0.1  0.4  0.   0.6  0.   0.4  0.5  0.   0.1  0.   0.1  0.7\n",
      "  0.8  0.3  0.   0.3  1.   0.4  0.9  0.7  0.6  0.5  0.   0.1  0.   0.   0.9\n",
      "  0.5  0.1  0.9  0.8  0.   0.7  0.9  0.3  0.   0.1  0.5  0.1  0.4  0.   0.1\n",
      "  0.8  0.4  0.   0.   0.7  0.5  0.2  0.   0.6  0.9  0.6  0.   0.8  0.3  0.8\n",
      "  0.   0.7  0.8  0.5  0.8  0.7  0.8  0.   0.7  1.   0.8  0.7  0.7  0.8  0.4\n",
      "  0.9  0.7  0.8  0.6  0.1  0.8  0.9  0.4  0.8  0.8  0.5  0.7  0.5  0.2  0.7\n",
      "  0.8  0.7  0.7  0.9  0.8  0.7  0.5  0.6  0.   0.6  0.6  0.   0.8  0.   0.9\n",
      "  0.   0.3  1.   0.1  0.2  0.6  0.5  0.8  0.   0.7  0.5  0.4  0.6  0.9  0.\n",
      "  0.2  0.6  0.7  0.9  0.   0.   0.8  0.9  0.5  0.8  1.   0.4  0.4  0.4  0.6\n",
      "  0.   0.4  0.1  0.4  0.1  0.5  0.9  0.8  0.6  0.8  0.9  0.7  0.7  0.4  0.\n",
      "  1.   0.   0.5  0.9  0.   0.   0.7  0.4  0.8  0.7  0.1  0.5  0.6  0.8  0.\n",
      "  0.4  1.   0.4  0.9  0.9  0.   0.9  0.7  0.1  0.6  0.1  0.8  0.1  0.   0.8\n",
      "  0.4  0.6  0.2  0.   0.   0.5  0.5  0.   0.6  0.   0.2  0.4  0.   0.6  0.9\n",
      "  0.7  0.9  0.9  0.3  0.   0.4  1.   0.6  0.7  0.9  0.5  0.9  1.   0.9  0.\n",
      "  0.   0.3  0.7  0.4  0.7  1.   0.8  0.3  0.   0.4  0.8  0.7  0.5  0.4  0.9\n",
      "  0.8  0.8  0.2  0.7  0.7  0.1  0.   0.8  0.6  0.   0.7  0.6  0.4  0.   0.\n",
      "  0.7  0.1  0.6  0.9  0.4  0.5  1.   0.   0.6  0.5  0.5  0.6  0.   0.9  0.9\n",
      "  0.4  0.   0.   0.9  0.5  0.   0.7  0.9  0.8  0.5  0.5  0.   0.2  0.   0.1\n",
      "  0.   0.7  0.8  0.   0.4  0.6  0.5  0.8  0.   0.9  0.4  0.4  0.8  0.4  0.9\n",
      "  1.   0.7  0.6  0.6  0.8  0.9  0.8  0.8  0.   0.4  0.6  0.8  0.3  0.8  0.8\n",
      "  0.8  0.5  0.7  0.   0.8  0.6  0.   0.8  0.   0.9  0.8  0.7  0.8  0.5  0.\n",
      "  0.   0.8  0.6  0.9  0.   0.5  0.5  0.9  0.   0.   1.   0.7  0.7  0.5  0.6\n",
      "  0.7  0.1  0.8  0.4  0.3  0.   0.   0.   0.4  0.8  0.   0.9  0.6  0.1  0.7\n",
      "  0.6  0.   0.7  0.   0.   0.8  0.7  0.5  1.   0.   0.2  1.   0.3  0.9  0.6\n",
      "  0.8  0.8  0.3  0.8  0.8  0.8  0.   0.7  0.   0.6  0.3  0.   1.   0.8  0.\n",
      "  0. ] [ 0.7  0.1  0.  ...,  0.9  0.   0. ]\n",
      "[+] Prediction success, time spent so far  0.07 sec\n",
      "[+] Results saved, time spent so far  0.08 sec\n",
      "[+] End cycle, remaining time 1199.72 sec\n",
      "=========== Jasmine Training cycle 1 ================\n",
      "[+] Number of estimators: 31962\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c7809b3d01ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multilabel.classification'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 273\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0mclasses_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alex/anaconda/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mergesort'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'quicksort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(datanames)>0:\n",
    "    vprint( verbose,  \"************************************************************************\")\n",
    "    vprint( verbose,  \"****** Attempting to copy files (from res/) for RESULT submission ******\")\n",
    "    vprint( verbose,  \"************************************************************************\")\n",
    "    OK = data_io.copy_results(datanames, res_dir, default_output_dir, verbose) # DO NOT REMOVE!\n",
    "    if OK: \n",
    "        vprint( verbose,  \"[+] Success\")\n",
    "        datanames = [] # Do not proceed with learning and testing\n",
    "    else:\n",
    "        vprint( verbose, \"======== Some missing results on current datasets!\")\n",
    "        vprint( verbose, \"======== Proceeding to train/test:\\n\")\n",
    "    # =================== End @RESULT SUBMISSION (KEEP THIS) ==================\n",
    "\n",
    "    # ================ @CODE SUBMISSION (SUBTITUTE YOUR CODE) ================= \n",
    "    overall_time_budget = 0\n",
    "    for basename in datanames: # Loop over datasets\n",
    "        \n",
    "        vprint( verbose,  \"************************************************\")\n",
    "        vprint( verbose,  \"******** Processing dataset \" + basename.capitalize() + \" ********\")\n",
    "        vprint( verbose,  \"************************************************\")\n",
    "        \n",
    "        # ======== Learning on a time budget:\n",
    "        # Keep track of time not to exceed your time budget. Time spent to inventory data neglected.\n",
    "        start = time.time()\n",
    "        \n",
    "        # ======== Creating a data object with data, informations about it\n",
    "        vprint( verbose,  \"======== Reading and converting data ==========\")\n",
    "        D = DataManager(basename, default_input_dir, replace_missing=True, filter_features=True, verbose=verbose)\n",
    "        print D\n",
    "        \n",
    "        # ======== Keeping track of time\n",
    "        if debug_mode<1:\n",
    "            time_budget = D.info['time_budget']        # <== HERE IS THE TIME BUDGET!\n",
    "        else:\n",
    "            time_budget = max_time\n",
    "        overall_time_budget = overall_time_budget + time_budget\n",
    "        time_spent = time.time() - start\n",
    "        vprint( verbose,  \"[+] Remaining time after reading data %5.2f sec\" % (time_budget-time_spent))\n",
    "        if time_spent >= time_budget:\n",
    "            vprint( verbose,  \"[-] Sorry, time budget exceeded, skipping this task\")\n",
    "            execution_success = False\n",
    "            continue\n",
    "        \n",
    "        # ========= Creating a model, knowing its assigned task from D.info['task'].\n",
    "        # The model can also select its hyper-parameters based on other elements of info.  \n",
    "        # vprint( verbose,  \"======== Creating model ==========\")\n",
    "        # M = MyAutoML(D.info, verbose, debug_mode)\n",
    "        # print M\n",
    "        \n",
    "        # ========= Iterating over learning cycles and keeping track of time\n",
    "        time_spent = time.time() - start\n",
    "        vprint( verbose,  \"[+] Remaining time after building model %5.2f sec\" % (time_budget-time_spent))        \n",
    "        if time_spent >= time_budget:\n",
    "            vprint( verbose,  \"[-] Sorry, time budget exceeded, skipping this task\")\n",
    "            execution_success = False\n",
    "            continue\n",
    "\n",
    "        time_budget = time_budget - time_spent # Remove time spent so far\n",
    "        start = time.time()              # Reset the counter\n",
    "        time_spent = 0                   # Initialize time spent learning\n",
    "        time_spent_last = 0                   # Initialize time spent learning\n",
    "        cycle = 0\n",
    "        GPU =  False\n",
    "        \n",
    "        while  cycle <= 1: #max_cycle:\n",
    "            begin = time.time()\n",
    "            vprint( verbose,  \"=========== \" + basename.capitalize() +\" Training cycle \" + str(cycle) +\" ================\") \n",
    "            n_estimators = 10\n",
    "            if cycle==1:\n",
    "                n_estimators = int((np.floor(time_budget / time_spent_last) - 1 ) * 2)\n",
    "                if n_estimators <= 0:\n",
    "                    break\n",
    "            vprint( verbose,  \"[+] Number of estimators: %d\" % (n_estimators))   \n",
    "            \n",
    "            K = D.info['target_num']\n",
    "            sparse = False\n",
    "            if D.info['is_sparse'] == 1:\n",
    "                sparse = True\n",
    "           \n",
    "                \n",
    "            task = D.info['task']\n",
    "\n",
    "            if task == 'binary.classification' or task == 'multiclass.classification':\n",
    "                if task == 'binary.classification' and GPU:\n",
    "                    Y = run_nn.fit_predict( D.data['X_train'], D.data['Y_train'], [D.data['X_valid'], D.data['X_test']], n_epochs = n_estimators/10)\n",
    "                    Y_valid = Y[0]\n",
    "                    Y_test =  Y[1]\n",
    "                    print Y_valid, Y_test\n",
    "                elif sparse:\n",
    "                    M = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=n_estimators/10).fit(D.data['X_train'], D.data['Y_train'])\n",
    "                else:\n",
    "                    M = RandomForestClassifier(n_estimators, random_state=1).fit(D.data['X_train'], D.data['Y_train'])\n",
    "            else:\n",
    "                vprint( verbose,  \"[-] task not recognised\")\n",
    "                break         \n",
    "            vprint( verbose,  \"[+] Fitting success, time spent so far %5.2f sec\" % (time.time() - start))\n",
    "            \n",
    "            \n",
    "            # Make predictions\n",
    "            if task == 'binary.classification' and not GPU:\n",
    "                Y_valid = M.predict_proba(D.data['X_valid'])[:, 1]\n",
    "                Y_test =  M.predict_proba(D.data['X_test'])[:, 1]\n",
    "                # print Y_valid, Y_test\n",
    "                     \n",
    "            if sparse:\n",
    "                if task == 'multilabel.classification' or task == 'multiclass.classification':\n",
    "                    eps = 0.001\n",
    "                    for i in range(len(Y_valid)):\n",
    "                        pos = np.argmax(Y_valid[i])\n",
    "                        Y_valid[i] += eps\n",
    "                        Y_valid[i][pos] -= K * eps\n",
    "                    for i in range(len(Y_test)):\n",
    "                        pos = np.argmax(Y_test[i])\n",
    "                        Y_test[i] += eps\n",
    "                        Y_test[i][pos] -= K * eps\n",
    "                    \n",
    "            \n",
    "            vprint( verbose,  \"[+] Prediction success, time spent so far %5.2f sec\" % (time.time() - start))\n",
    "            # Write results\n",
    "            filename_valid = basename + '_valid_' + str(cycle).zfill(3) + '.predict'\n",
    "            data_io.write(os.path.join(default_output_dir, filename_valid), Y_valid)\n",
    "            filename_test = basename + '_test_' + str(cycle).zfill(3) + '.predict'\n",
    "            data_io.write(os.path.join(default_output_dir,filename_test), Y_test)\n",
    "            vprint( verbose,  \"[+] Results saved, time spent so far %5.2f sec\" % (time.time() - start))         \n",
    "            time_spent = time.time() - start \n",
    "            vprint( verbose,  \"[+] End cycle, remaining time %5.2f sec\" % (time_budget-time_spent))\n",
    "            cycle += 1\n",
    "            time_spent_last = time.time() - begin\n",
    "            time_budget = time_budget - time_spent_last # Remove time spent so far\n",
    "            \n",
    "    if zipme and not(running_on_codalab):\n",
    "        vprint( verbose,  \"========= Zipping this directory to prepare for submit ==============\")\n",
    "        data_io.zipdir(submission_filename + '.zip', \".\")\n",
    "    \t\n",
    "    overall_time_spent = time.time() - overall_start\n",
    "    if execution_success:\n",
    "        vprint( verbose,  \"[+] Done\")\n",
    "        vprint( verbose,  \"[+] Overall time spent %5.2f sec \" % overall_time_spent + \"::  Overall time budget %5.2f sec\" % overall_time_budget)\n",
    "    else:\n",
    "        vprint( verbose,  \"[-] Done, but some tasks aborted because time limit exceeded\")\n",
    "        vprint( verbose,  \"[-] Overall time spent %5.2f sec \" % overall_time_spent + \" > Overall time budget %5.2f sec\" % overall_time_budget)\n",
    "              \n",
    "    if running_on_codalab: \n",
    "        if execution_success:\n",
    "            exit(0)\n",
    "        else:\n",
    "            exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
